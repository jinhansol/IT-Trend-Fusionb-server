# backend/services/career_service.py
# flake8: noqa

from collections import Counter
from datetime import datetime, timedelta
from typing import List, Optional, Any

from sqlalchemy.orm import Session

from database.mariadb import SessionLocal
from database.models import CareerJob, UserProfile


###########################################
# ğŸ” ê¸°ìˆ  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
###########################################
TECH_KEYWORDS = [
    "Python", "React", "Node", "TypeScript", "Vue",
    "Next.js", "Java", "Spring", "Django", "Flutter",
    "AWS", "Kubernetes", "Docker", "AI", "ML", "Data",
]


###########################################
# ğŸ§© ì œëª© ê¸°ë°˜ ê¸°ìˆ  ì¶”ì¶œ
###########################################
def extract_skills_from_title(title: Optional[str]) -> List[str]:
    found: List[str] = []
    if not title:
        return found
    lower_title = title.lower()
    for skill in TECH_KEYWORDS:
        if skill.lower() in lower_title:
            found.append(skill)
    return found


###########################################
# ğŸ· íƒœê·¸ ì •ê·œí™”
###########################################
def normalize_tags(raw_tags: Any, title: Optional[str] = None) -> List[str]:
    tags: List[str] = []

    # 1) ê¸°ë³¸ tag ì²˜ë¦¬
    if isinstance(raw_tags, list):
        for t in raw_tags:
            if isinstance(t, str) and t.strip():
                tags.append(t.strip())

    elif isinstance(raw_tags, str):
        for t in raw_tags.split(","):
            tag = t.strip()
            if tag:
                tags.append(tag)

    # 2) ì œëª©ì—ì„œ ê¸°ìˆ  ìŠ¤í‚¬ ìë™ ì¶”ì¶œ
    if title:
        tags.extend(extract_skills_from_title(title))

    # 3) ì†Œë¬¸ì ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    unique = {}
    for t in tags:
        key = t.lower()
        if key not in unique:
            unique[key] = t

    return list(unique.values())


###########################################
# â³ posted_date íŒŒì‹±
###########################################
def parse_posted_date(raw: Any) -> datetime:
    if isinstance(raw, datetime):
        return raw

    if isinstance(raw, str):
        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d"):
            try:
                return datetime.strptime(raw, fmt)
            except ValueError:
                continue

        try:
            return datetime.fromisoformat(raw)
        except Exception:
            pass

    return datetime.utcnow()


###########################################
# ğŸ“Š ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„ (8ì£¼)
###########################################
def get_weekly_tech_trends(db: Session, weeks: int = 8):
    end_date = datetime.now()
    start_date = end_date - timedelta(weeks=weeks)

    jobs = (
        db.query(CareerJob)
        .filter(CareerJob.posted_date >= start_date)
        .all()
    )

    weekly_counter = Counter()

    for job in jobs:
        skills = extract_skills_from_title(job.title)

        if job.tags:
            if isinstance(job.tags, list):
                for t in job.tags:
                    if isinstance(t, str):
                        skills.append(t)

        weekly_counter.update(skills)

    return [
        {"skill": skill, "count": count}
        for skill, count in weekly_counter.most_common()
    ]


###########################################
# ğŸ¯ ì‚¬ìš©ì ìŠ¤í‚¬ ì¶”ì¶œ
###########################################
def get_user_skills(user: UserProfile):
    if not user:
        return []

    raw = user.tech_stack

    if isinstance(raw, list):
        return [s for s in raw if isinstance(s, str) and s.strip()]

    if isinstance(raw, str):
        return [s.strip() for s in raw.split(",") if s.strip()]

    return []


###########################################
# ğŸ¯ ì‚¬ìš©ì ë§ì¶¤ ì±„ìš© ì¶”ì²œ
###########################################
def get_recommended_jobs(db: Session, skills: list, limit: int = 20):
    if not skills:
        return (
            db.query(CareerJob)
            .order_by(CareerJob.posted_date.desc())
            .limit(limit)
            .all()
        )

    query = db.query(CareerJob)
    filters = []

    for skill in skills:
        if skill:
            filters.append(CareerJob.title.ilike(f"%{skill}%"))

    if filters:
        query = query.filter(*filters)

    return (
        query.order_by(CareerJob.posted_date.desc())
        .limit(limit)
        .all()
    )


###########################################
# ğŸ’¾ CareerJob ë‹¨ì¼ ì €ì¥
###########################################
def save_job_posting(db: Session, job_data: dict):
    url = job_data.get("url")
    if not url:
        return False

    # ì¤‘ë³µ ì²´í¬
    exists = (
        db.query(CareerJob)
        .filter(CareerJob.url == url)
        .first()
    )
    if exists:
        return False

    title = job_data.get("title")
    norm_tags = normalize_tags(job_data.get("tags"), title=title)

    job = CareerJob(
        title=title,
        company=job_data.get("company"),
        location=job_data.get("location"),
        job_type=job_data.get("job_type"),
        url=url,
        tags=norm_tags,
        source=job_data.get("source", "Unknown"),
        posted_date=parse_posted_date(job_data.get("posted_date")),
        created_at=datetime.utcnow(),
    )

    try:
        db.add(job)
        db.commit()
        return True
    except Exception as e:
        print("âŒ CareerJob Insert Error:", e)
        db.rollback()
        return False


###########################################
# ğŸ’¾ ë¦¬ìŠ¤íŠ¸ ì¼ê´„ ì €ì¥
###########################################
def save_crawled_jobs(results: List[dict]):
    db = SessionLocal()
    saved = 0

    for job in results:
        if save_job_posting(db, job):
            saved += 1

    db.close()
    print(f"ğŸ’¾ CareerJob ì €ì¥ ì™„ë£Œ: {saved}ê°œ / ì´ {len(results)}ê°œ")


###########################################
# ğŸ”¥ NEWS ìŠ¤íƒ€ì¼ Career Pipeline (ë‹¨ì¼ íŒŒì¼ ë°©ì‹)
###########################################
def run_career_pipeline():
    print("\nğŸ”¥ CAREER PIPELINE START")

    # 1) JobKorea
    try:
        from services.jobkorea_scraper import crawl_jobkorea
        jk = crawl_jobkorea()
        print(f"ğŸ“Œ JobKorea í™•ë³´: {len(jk)}ê°œ")
    except Exception as e:
        print(f"âŒ JobKorea í¬ë¡¤ë§ ì˜¤ë¥˜:", e)
        jk = []

    # 2) Saramin
    try:
        from services.saramin_scraper import crawl_saramin
        sm = crawl_saramin()
        print(f"ğŸ“Œ Saramin í™•ë³´: {len(sm)}ê°œ")
    except Exception as e:
        print(f"âŒ Saramin í¬ë¡¤ë§ ì˜¤ë¥˜:", e)
        sm = []

    # 3) ì €ì¥
    all_jobs = jk + sm
    save_crawled_jobs(all_jobs)

    print(f"ğŸ’¾ CareerJob ì €ì¥ ì™„ë£Œ: {len(all_jobs)}ê°œ í¬ë¡¤ë§ë¨")
    print("ğŸ”¥ CAREER PIPELINE END\n")

###########################################
# â­ ì‹ ê·œ: ì±„ìš© ê³µê³  í˜ì´ì§•
###########################################
def get_jobs_paged(db: Session, page: int, size: int):
    """
    ì±„ìš© ê³µê³  í˜ì´ì§• í•¨ìˆ˜
    /api/career/jobs?page=1&size=6ì—ì„œ ì‚¬ìš©ë¨
    """

    total = db.query(CareerJob).count()

    jobs = (
        db.query(CareerJob)
        .order_by(CareerJob.posted_date.desc())
        .offset((page - 1) * size)
        .limit(size)
        .all()
    )

    return {
        "page": page,
        "size": size,
        "total": total,
        "total_pages": (total + size - 1) // size,
        "jobs": jobs,
    }
