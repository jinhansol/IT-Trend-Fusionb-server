"""
AI í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ (ì–‘ìª½ ë””ë²„ê¹… í™œì„±í™” ë²„ì „)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Google ë‰´ìŠ¤: ë¦¬ë””ë ‰ì…˜ + OG ì´ë¯¸ì§€ + ìƒì„¸ ë¡œê·¸
âœ… Naver ë‰´ìŠ¤: OG ì´ë¯¸ì§€ ì‹œë„ + ì¸ë„¤ì¼ ì¶”ì¶œ ë¡œê·¸
âœ… ë””ë²„ê¹… ë ˆë²¨ í™•ì¥: ê° ë‹¨ê³„ë³„ ì¶œë ¥
"""

import os
import requests
import feedparser
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸŒ¿ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§© ê³µí†µ: OpenGraph ì´ë¯¸ì§€ ì¶”ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_news_thumbnail(url: str) -> str:
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, headers=headers, timeout=6)
        if not res.ok:
            print(f" âŒ [ì¸ë„¤ì¼] ìš”ì²­ ì‹¤íŒ¨ ({res.status_code}) â†’ {url}")
            return ""

        soup = BeautifulSoup(res.text, "html.parser")
        image_candidates = []

        # â‘  OG/Twitter ì´ë¯¸ì§€ íƒìƒ‰
        for prop in ["og:image", "twitter:image", "image"]:
            tag = soup.find("meta", property=prop) or soup.find("meta", attrs={"name": prop})
            if tag and tag.get("content"):
                image_candidates.append(tag["content"])

        # â‘¡ Fallback: ì²« ë²ˆì§¸ <img> íƒœê·¸
        if not image_candidates:
            img_tag = soup.find("img")
            if img_tag and img_tag.get("src"):
                from urllib.parse import urljoin
                img_src = img_tag["src"]
                if img_src.startswith("//"):
                    img_src = "https:" + img_src
                elif img_src.startswith("/"):
                    img_src = urljoin(url, img_src)
                image_candidates.append(img_src)

        # â‘¢ ìœ íš¨ ì´ë¯¸ì§€ í•„í„°ë§
        for img_url in image_candidates:
            if img_url.startswith("http"):
                try:
                    check = requests.head(img_url, headers=headers, timeout=3)
                    if check.ok and "image" in check.headers.get("Content-Type", ""):
                        print(f" âœ… [ì¸ë„¤ì¼] ê°ì§€ë¨ â†’ {img_url}")
                        return img_url
                except:
                    print(f" âš ï¸ [ì¸ë„¤ì¼] MIME í™•ì¸ ì‹¤íŒ¨ â†’ {img_url}")
                    return img_url

        print(" âš ï¸ [ì¸ë„¤ì¼] ìœ íš¨ ì´ë¯¸ì§€ ì—†ìŒ")
    except Exception as e:
        print(f"âš ï¸ [ì¸ë„¤ì¼ ì˜¤ë¥˜] {e}")
        return ""
    return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¨ AI ì´ë¯¸ì§€ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_ai_thumbnail(title: str, summary: str) -> str:
    try:
        prompt = f"""
        Create a minimalistic tech news illustration about:
        "{title}" â€” {summary}.
        Style: flat, clean, futuristic, blue accent.
        """
        res = client.images.generate(
            model="gpt-image-1",
            prompt=prompt,
            size="512x512"
        )
        return res.data[0].url
    except Exception:
        return "https://cdn-icons-png.flaticon.com/512/2965/2965879.png"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§  ì˜ì–´ â†’ í•œêµ­ì–´ ìš”ì•½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate_summary(summary: str) -> str:
    if not summary:
        return "ë‚´ìš© ì—†ìŒ"
    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ìµœì‹  IT ë‰´ìŠ¤ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ëŠ” AIì•¼."},
                {"role": "user", "content": f"ë‹¤ìŒ ë¬¸ì¥ì„ 2ë¬¸ì¥ ì´ë‚´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì¤˜:\n{summary}"}
            ],
            max_tokens=150
        )
        return res.choices[0].message.content.strip()
    except Exception:
        return summary


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸŒ Google ë‰´ìŠ¤ ìˆ˜ì§‘ (ë””ë²„ê¹… í¬í•¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_google_news(limit: int = 4):
    print("\nğŸŒ [Google News] ìˆ˜ì§‘ ì‹œì‘")
    url = "https://news.google.com/rss/search?q=technology&hl=en&gl=US&ceid=US:en"
    feed = feedparser.parse(url)
    results = []

    google_session = requests.Session()
    google_session.headers.update({"User-Agent": "Mozilla/5.0"})

    for idx, entry in enumerate(feed.entries[:limit]):
        title = getattr(entry, "title", "ì œëª© ì—†ìŒ")
        summary_raw = getattr(entry, "summary", "ë‚´ìš© ì—†ìŒ")
        summary_kr = translate_summary(summary_raw)
        raw_link = getattr(entry, "link", "#")

        print(f"\nğŸ“° [Google-{idx+1}] {title[:60]}")
        print(f" - RSS ë§í¬: {raw_link}")

        # ì‹¤ì œ ë‰´ìŠ¤ URL ì°¾ê¸°
        real_link = raw_link
        if hasattr(entry, "source") and hasattr(entry.source, "href"):
            real_link = entry.source.href
            print(f" - RSS ë‚´ë¶€ source ë§í¬ ì‚¬ìš©: {real_link}")
        else:
            try:
                redirect_res = google_session.get(raw_link, timeout=6, allow_redirects=True)
                if redirect_res.status_code == 200 and "news.google.com" not in redirect_res.url:
                    real_link = redirect_res.url
                    print(f" - ë¦¬ë””ë ‰ì…˜ ì„±ê³µ â†’ {real_link}")
                else:
                    print(" âš ï¸ ë¦¬ë””ë ‰ì…˜ ì‹¤íŒ¨ (Google ë‚´ë¶€ ë§í¬ ìœ ì§€)")
            except Exception as e:
                print(f" âš ï¸ ë¦¬ë””ë ‰ì…˜ ì˜¤ë¥˜: {e}")

        # ì¸ë„¤ì¼ ì‹œë„
        image = get_news_thumbnail(real_link)
        if not image:
            print(" âš ï¸ ì¸ë„¤ì¼ ì—†ìŒ â†’ AI ìƒì„± ì¤‘...")
            image = generate_ai_thumbnail(title, summary_kr)

        results.append({
            "source": "Google News",
            "title": title,
            "summary": summary_kr,
            "url": real_link,
            "published": getattr(entry, "published", "N/A"),
            "image": image,
        })

    google_session.close()
    print(f"\nâœ… [Google News] {len(results)}ê°œ ì™„ë£Œ")
    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ‡°ğŸ‡· Naver ë‰´ìŠ¤ ìˆ˜ì§‘ (ë””ë²„ê¹… ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ‡°ğŸ‡· Naver ë‰´ìŠ¤ ìˆ˜ì§‘ (ë””ë²„ê¹… + ë¦¬ë””ë ‰ì…˜ ì¶”ì  ê°•í™”)
def fetch_naver_news(keyword: str = "IT ê¸°ìˆ ", limit: int = 4):
    print(f"\nğŸ‡°ğŸ‡· [Naver News] ìˆ˜ì§‘ ì‹œì‘ â€” í‚¤ì›Œë“œ: {keyword}")
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
        "User-Agent": "Mozilla/5.0",
    }
    params = {"query": keyword, "display": 10, "sort": "date"}

    try:
        with requests.Session() as s:
            res = s.get(url, headers=headers, params=params, timeout=5)
            res.raise_for_status()
            items = res.json().get("items", [])
    except Exception as e:
        print(f"âŒ Naver API ì‹¤íŒ¨: {e}")
        return []

    results = []
    for i, item in enumerate(items[:limit]):
        title = item["title"].replace("<b>", "").replace("</b>", "")
        summary = item["description"].replace("<b>", "").replace("</b>", "")
        link = item["link"]

        print(f"\nğŸ—ï¸ [Naver-{i+1}] {title[:60]}")
        print(f" - ì›ë³¸ ë§í¬: {link}")

        # âœ… ë¦¬ë””ë ‰ì…˜ ì¶”ì  (naver ì¤‘ê³„ í˜ì´ì§€ì¸ ê²½ìš° ì‹¤ì œ ì–¸ë¡ ì‚¬ë¡œ ì´ë™)
        try:
            redirect_res = s.get(link, timeout=6, allow_redirects=True)
            real_link = redirect_res.url if redirect_res.status_code == 200 else link
            print(f" - ìµœì¢… ë§í¬: {real_link}")
        except Exception as e:
            print(f" âš ï¸ ë¦¬ë””ë ‰ì…˜ ì‹¤íŒ¨: {e}")
            real_link = link

        # âœ… ì¸ë„¤ì¼ ì‹œë„
        image = get_news_thumbnail(real_link)
        if image:
            print(f" âœ… ì¸ë„¤ì¼ ê°ì§€: {image}")
        else:
            print(" âš ï¸ ì¸ë„¤ì¼ ì—†ìŒ â†’ AI ëŒ€ì²´ ìƒì„± ì¤‘...")
            image = generate_ai_thumbnail(title, summary)

        results.append({
            "source": "Naver News",
            "title": title,
            "summary": summary,
            "url": real_link,
            "published": item.get("pubDate", "N/A"),
            "image": image,
        })

    print(f"\nâœ… [Naver News] {len(results)}ê°œ ì™„ë£Œ")
    return results



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“° í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_latest_news(keyword: str = "IT ê¸°ìˆ ", limit: int = 4):
    google_news = fetch_google_news(limit)
    naver_news = fetch_naver_news(keyword, limit)
    combined = google_news + naver_news
    print(f"\nğŸ§© í†µí•© ë‰´ìŠ¤ ì´ {len(combined)}ê°œ ë°˜í™˜ ì™„ë£Œ")
    return combined
