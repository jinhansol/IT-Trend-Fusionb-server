# flake8: noqa
"""
ğŸ”¥ IT ë‰´ìŠ¤ í†µí•© í¬ë¡¤ë§ + AI ìš”ì•½/ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œ + DB ì €ì¥ (ìµœì¢… ì•ˆì •í™” ë²„ì „ â€” ë§¤ì²´ë‹¹ 3ê°œ ì œí•œ)
"""

import os
import time
import json
import requests
import feedparser
from datetime import datetime
from typing import List, Dict
from urllib.parse import urlparse, urlunparse

from dotenv import load_dotenv
from bs4 import BeautifulSoup
from openai import OpenAI

from database.mariadb import SessionLocal
from database.models import NewsFeed

load_dotenv()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ----------------------------------------------------------
# 1. RSS ì†ŒìŠ¤ (êµ­ë‚´ + í•´ì™¸)
# ----------------------------------------------------------

IT_FEEDS = [
    # ğŸ‡°ğŸ‡· ZDNet
    "https://www.zdnet.co.kr/Include/news.xml",
    "https://www.zdnet.co.kr/Include/news_ai.xml",
    "https://www.zdnet.co.kr/Include/news_cloud.xml",
    "https://www.zdnet.co.kr/Include/news_security.xml",

    # ğŸ‡°ğŸ‡· ETNews
    "https://rss.etnews.com/Section903.xml",
    "https://rss.etnews.com/AI.xml",
    "https://rss.etnews.com/Cloud.xml",
    "https://rss.etnews.com/Security.xml",
    "https://rss.etnews.com/Semicon.xml",

    # ğŸ‡°ğŸ‡· ê¸°íƒ€
    "https://www.itworld.co.kr/rss/all.xml",
    "https://www.ciokorea.com/rss/all.xml",
    "https://koreaittimes.com/rss/allArticle.xml",
    "https://www.ddaily.co.kr/news/rss/allArticle.xml",
    "https://www.bloter.net/rss",
    "https://www.boannews.com/media/rss.xml",

    # ğŸ‡ºğŸ‡¸ í•´ì™¸ IT ì „ë¬¸
    "https://techcrunch.com/feed/",
    "https://www.wired.com/feed/category/business/latest/rss",
    "https://www.theverge.com/rss/index.xml",
    "http://feeds.arstechnica.com/arstechnica/index",
    "https://venturebeat.com/feed/",
    "https://feeds.infoq.com/",
    "http://rss.slashdot.org/Slashdot/slashdotMain",
]


# ----------------------------------------------------------
# URL ì •ê·œí™” (ì¿¼ë¦¬/í•´ì‹œ ì œê±°)
# ----------------------------------------------------------
def normalize_url(url: str) -> str:
    try:
        p = urlparse(url)
        return urlunparse(p._replace(query="", fragment=""))
    except:
        return url


# ----------------------------------------------------------
# RSS â†’ ë§¤ì²´ë³„ ê¸°ì‚¬ ê·¸ë£¹í™”
# ----------------------------------------------------------
def fetch_grouped_rss() -> Dict[str, List[Dict]]:
    grouped = {}

    for feed_url in IT_FEEDS:
        parsed = feedparser.parse(feed_url)
        source = urlparse(feed_url).netloc

        if source not in grouped:
            grouped[source] = []

        for entry in parsed.entries:
            url = entry.get("link", "").strip()
            title = entry.get("title", "").strip()
            if not url or not title:
                continue

            grouped[source].append({
                "title": title,
                "url": normalize_url(url),
                "summary": entry.get("summary", "").strip(),
                "published": entry.get("published", ""),
                "source": source,
            })

    return grouped


# ----------------------------------------------------------
# ë§¤ì²´ë‹¹ ìµœì‹  Nê°œë§Œ ì„ íƒ
# ----------------------------------------------------------
MAX_PER_SOURCE = 3

def get_limited_items() -> List[Dict]:
    grouped = fetch_grouped_rss()
    limited = []

    for source, items in grouped.items():
        # published ê¸°ì¤€ìœ¼ë¡œ ìµœì‹ ìˆœ ì •ë ¬
        sorted_items = sorted(
            items,
            key=lambda x: x["published"] or "",
            reverse=True
        )

        limited.extend(sorted_items[:MAX_PER_SOURCE])

    return limited


# ----------------------------------------------------------
# ë³¸ë¬¸ í¬ë¡¤ë§
# ----------------------------------------------------------
def clean_text(text: str) -> str:
    return "\n".join([t.strip() for t in text.split("\n") if t.strip()])


def fetch_article_content(url: str) -> str:
    try:
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=12)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        selectors = [
            "article", "#articleBody", "#articleBodyContents",
            ".article_body", ".art_txt", "section.article"
        ]

        for sel in selectors:
            node = soup.select_one(sel)
            if node:
                return clean_text(node.get_text(separator="\n"))

        return clean_text(soup.get_text(separator="\n"))

    except Exception as e:
        print(f"[ERROR] fetch_article_content: {e}")
        return ""


# ----------------------------------------------------------
# AI ìš”ì•½
# ----------------------------------------------------------
def analyze_article(title: str, content: str):
    prompt = f"""
    ì•„ë˜ëŠ” IT ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ì…ë‹ˆë‹¤.

    ì œëª©:
    {title}

    ë³¸ë¬¸(ìš”ì•½ìš©):
    {content[:4000]}

    ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”:

    {{
      "summary": "...",
      "category": "AI/ë³´ì•ˆ/ëª¨ë°”ì¼/í´ë¼ìš°ë“œ/ì •ì±…/ìŠ¤íƒ€íŠ¸ì—…/ê¸°íƒ€ ì¤‘ í•˜ë‚˜",
      "keywords": ["...", "...", "..."]
    }}
    """
    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )

        raw = res.choices[0].message.content
        cleaned = raw.replace("```json", "").replace("```", "").strip()
        return json.loads(cleaned)

    except:
        return {"summary": "", "category": "ê¸°íƒ€", "keywords": ["IT"]}


# ----------------------------------------------------------
# ë‚ ì§œ íŒŒì‹±
# ----------------------------------------------------------
def parse_published_date(raw: str):
    try:
        parsed = feedparser._parse_date(raw)
        if parsed:
            return datetime(*parsed[:6])
    except:
        pass
    return datetime.utcnow()


# ----------------------------------------------------------
# DB ì¤‘ë³µ ì²´í¬
# ----------------------------------------------------------
def exists_in_db(url, title):
    db = SessionLocal()
    exists = (
        db.query(NewsFeed)
        .filter((NewsFeed.url == url) | (NewsFeed.title == title))
        .first()
    )
    db.close()
    return exists


# ----------------------------------------------------------
# DB ì €ì¥
# ----------------------------------------------------------
def save_news(item, ai, content):
    db = SessionLocal()
    try:
        news = NewsFeed(
            title=item["title"],
            summary=ai.get("summary", ""),
            content=content,
            category=ai.get("category", "ê¸°íƒ€"),
            keywords=json.dumps(ai.get("keywords", ["IT"]), ensure_ascii=False),
            source=item["source"],
            url=item["url"],
            published_at=parse_published_date(item["published"]),
            created_at=datetime.utcnow(),
        )
        db.add(news)
        db.commit()
        print(" - ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        print(f"[ERROR] DB ì €ì¥ ì‹¤íŒ¨: {e}")
        db.rollback()

    finally:
        db.close()


# ----------------------------------------------------------
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ----------------------------------------------------------
def run_news_pipeline():
    limited_items = get_limited_items()
    print(f"[INFO] ë§¤ì²´ë‹¹ 3ê°œ ì œí•œ â†’ ì´ {len(limited_items)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")

    for idx, item in enumerate(limited_items, start=1):
        print(f"\n[{idx}/{len(limited_items)}] {item['title']}")

        if exists_in_db(item["url"], item["title"]):
            print(" - DB ì¤‘ë³µ â†’ Skip")
            continue

        content = fetch_article_content(item["url"])
        if len(content) < 150:
            print(" - ë³¸ë¬¸ ë¶€ì¡± â†’ Skip")
            continue

        ai = analyze_article(item["title"], content)
        save_news(item, ai, content)

        time.sleep(0.8)
