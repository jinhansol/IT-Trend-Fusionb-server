# backend/services/news_service.py
# flake8: noqa
"""
ğŸ”¥ FINAL v3 â€” 20ê°œ RSS ìœ ì§€ / ë§¤ì²´ë‹¹ 3ê°œ í™•ë³´ / HTML fallback ê°•í™”
ğŸ”¥ Skip ZERO / ë³¸ë¬¸ ë¶€ì¡± ê°•ì œ ìš”ì•½ / URL ì˜¤ë¥˜ ì™„ì „ í•´ê²°
ğŸ”¥ í•œêµ­ì–´ ìë™ ìš”ì•½ ì•ˆì •í™” / IT í•„í„° ê°œì„ 
"""

import os
import json
import time
import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse, urljoin
from dotenv import load_dotenv
from openai import OpenAI

from database.mariadb import SessionLocal
from database.models import NewsFeed

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# -------------------------------
# 20ê°œ RSS (ì±„ì€ ìš”êµ¬ì‚¬í•­ ê·¸ëŒ€ë¡œ)
# -------------------------------
IT_FEEDS = [
    "https://www.zdnet.co.kr/Include/news.xml",
    "https://www.zdnet.co.kr/Include/news_ai.xml",
    "https://www.zdnet.co.kr/Include/news_cloud.xml",
    "https://www.zdnet.co.kr/Include/news_security.xml",

    "https://rss.etnews.com/Section903.xml",
    "https://rss.etnews.com/AI.xml",
    "https://rss.etnews.com/Cloud.xml",
    "https://rss.etnews.com/Security.xml",
    "https://rss.etnews.com/Semicon.xml",

    "https://www.itworld.co.kr/rss/all.xml",
    "https://www.ciokorea.com/rss/all.xml",
    "https://koreaittimes.com/rss/allArticle.xml",
    "https://www.ddaily.co.kr/news/rss/allArticle.xml",
    "https://www.bloter.net/rss",
    "https://www.boannews.com/media/rss.xml",

    "https://techcrunch.com/feed/",
    "https://www.wired.com/feed/category/business/latest/rss",
    "https://www.theverge.com/rss/index.xml",
    "http://feeds.arstechnica.com/arstechnica/index",
    "https://venturebeat.com/feed/",
    "https://feeds.infoq.com/",
    "http://rss.slashdot.org/Slashdot/slashdotMain",
]

# -------------------------------
# ë„ë©”ì¸ ë§¤í•‘
# -------------------------------
FALLBACK_MAP = {
    "zdnet.co.kr": "https://www.zdnet.co.kr/news/",
    "etnews.com": "https://www.etnews.com/news/",
    "itworld.co.kr": "https://www.itworld.co.kr/",
    "ciokorea.com": "https://www.ciokorea.com/",
    "koreaittimes.com": "https://koreaittimes.com/",
    "ddaily.co.kr": "https://www.ddaily.co.kr/news/",
    "bloter.net": "https://www.bloter.net/news",
    "boannews.com": "https://www.boannews.com/media/t_list.asp",

    "techcrunch.com": "https://techcrunch.com/",
    "wired.com": "https://www.wired.com/business/",
    "theverge.com": "https://www.theverge.com/tech",
    "arstechnica.com": "https://arstechnica.com/",
    "venturebeat.com": "https://venturebeat.com/category/ai/",
    "infoq.com": "https://www.infoq.com/",
    "slashdot.org": "https://slashdot.org/",
}

# -------------------------------
# RSS íŒŒì‹±
# -------------------------------
def fetch_rss(feed_url):
    parsed = feedparser.parse(feed_url)
    items = []

    for e in parsed.entries:
        title = e.get("title", "").strip()
        link = e.get("link", "").strip()

        if not title or not link:
            continue

        items.append({
            "title": title,
            "url": link,
            "published": e.get("published", ""),
            "source": feed_url
        })

    return items

# -------------------------------
# HTML fallback
# -------------------------------
def fetch_html_items(feed_url):
    domain = urlparse(feed_url).netloc.replace("www.", "")

    base_url = None
    for key in FALLBACK_MAP:
        if key in domain:
            base_url = FALLBACK_MAP[key]
            break

    if not base_url:
        return []

    try:
        res = requests.get(base_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=12)
        soup = BeautifulSoup(res.text, "html.parser")

        items = []
        for a in soup.find_all("a", href=True)[:30]:
            title = a.get_text(strip=True)
            link = a["href"]

            if len(title) < 6:
                continue

            full = link if link.startswith("http") else urljoin(base_url, link)

            items.append({
                "title": title,
                "url": full,
                "published": "",
                "source": base_url,
            })

        return items

    except:
        return []

# -------------------------------
# ë³¸ë¬¸ í¬ë¡¤ë§
# -------------------------------
ARTICLE_SELECTORS = [
    "article", "main", "#articleBody", "#articleBodyContents",
    ".article_body", ".art_txt", ".post-content", "section.article"
]

def fetch_content(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=12)
        soup = BeautifulSoup(res.text, "html.parser")

        for sel in ARTICLE_SELECTORS:
            t = soup.select_one(sel)
            if t:
                txt = t.get_text(separator="\n").strip()
                if len(txt) > 80:
                    return txt
        return soup.get_text(separator="\n").strip()
    except:
        return ""

# -------------------------------
# IT í•„í„°
# -------------------------------
IT_KEYWORDS = [
    "ai", "gpt", "llm", "openai", "cloud", "security",
    "server", "backend", "frontend", "devops",
    "gpu", "cpu", "robot", "tech", "semiconductor",
    "ë°ì´í„°", "ë³´ì•ˆ", "ë°˜ë„ì²´", "ê°œë°œì"
]

def is_it_related(title, content):
    text = (title + " " + content).lower()
    # ì œëª© ê¸°ë°˜ ìš°ì„  í•„í„°(ë” ê°•í•˜ê²Œ ì ìš©)
    for kw in IT_KEYWORDS:
        if kw.lower() in title.lower():
            return True

    # ë³¸ë¬¸ ê¸°ë°˜ ë³´ì¡° í•„í„°
    for kw in IT_KEYWORDS:
        if kw.lower() in text:
            return True

    return False

# -------------------------------
# AI ìš”ì•½
# -------------------------------
def analyze_article(title, content):
    if len(content) < 50:
        content = f"[ë³¸ë¬¸ ë¶€ì¡±] ì œëª©ë§Œ ê¸°ë°˜ìœ¼ë¡œ IT ìš”ì•½ ìƒì„±: {title}"

    prompt = f"""
ë‹¹ì‹ ì€ IT ë‰´ìŠ¤ ì „ë¬¸ ìš”ì•½ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

[ì œëª©]
{title}

[ë³¸ë¬¸]
{content[:2500]}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
{{
  "summary": "í•œêµ­ì–´ 3~4ë¬¸ì¥ ìš”ì•½",
  "category": "ai|cloud|security|backend|frontend|data|robotics|etc",
  "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
}}
"""

    # ------ 1st Try ------
    try:
        res = client.responses.create(
            model="gpt-4o-mini",
            input=prompt
        )
        raw = res.output_text.strip().replace("```json", "").replace("```", "")
        return json.loads(raw)
    except:
        pass

    # ------ Retry ------
    try:
        res = client.responses.create(
            model="gpt-4o-mini",
            input=prompt + "\nJSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."
        )
        raw = res.output_text.strip().replace("```json", "").replace("```", "")
        return json.loads(raw)
    except:
        return {"summary": title, "category": "etc", "keywords": ["IT"]}

# -------------------------------
# DB
# -------------------------------
def exists(url, title):
    db = SessionLocal()
    row = (
        db.query(NewsFeed)
        .filter((NewsFeed.url == url) | (NewsFeed.title == title))
        .first()
    )
    db.close()
    return row

def save(item, ai, content):
    db = SessionLocal()
    try:
        news = NewsFeed(
            title=item["title"],
            summary=ai["summary"],
            content=content,
            category=ai["category"],
            keywords=json.dumps(ai["keywords"], ensure_ascii=False),
            url=item["url"],
            source=urlparse(item["url"]).netloc,   # RSS ì£¼ì†Œê°€ ì•„ë‹ˆë¼ ê¸°ì‚¬ ë„ë©”ì¸ë§Œ
            published_at=datetime.utcnow(),
            created_at=datetime.utcnow(),
        )
        db.add(news)
        db.commit()
    except Exception as e:
        print("âŒ DB ì €ì¥ ì‹¤íŒ¨:", e)
        db.rollback()
    finally:
        db.close()

# -------------------------------
# PIPELINE
# -------------------------------
def run_news_pipeline():
    print("\nğŸ”¥ NEWS PIPELINE START")

    all_items = []

    # ê° RSSì—ì„œ 3ê°œ í™•ë³´
    for feed in IT_FEEDS:
        rss = fetch_rss(feed)

        if len(rss) < 3:
            html = fetch_html_items(feed)
            rss.extend(html)

        all_items.extend(rss[:3])

    print(f"ğŸ“Œ 1ì°¨ í™•ë³´: {len(all_items)}")

    # ë¶€ì¡±í•˜ë©´ ë³´ì¶©(ìµœì†Œ 60ê°œ)
    if len(all_items) < 60:
        need = 60 - len(all_items)
        print(f"âš ï¸ ë¶€ì¡± {need}ê°œ â†’ fallback ì¶”ê°€ í™•ë³´ ì‹œì‘")

        for feed in IT_FEEDS:
            extra = fetch_html_items(feed)
            for ex in extra:
                if need <= 0:
                    break
                all_items.append(ex)
                need -= 1
            if need <= 0:
                break

    print(f"âœ… ìµœì¢… í™•ë³´: {len(all_items)}ê°œ\n")

    # ë¶„ì„ + ì €ì¥
    for idx, item in enumerate(all_items, start=1):
        print(f"[{idx}/{len(all_items)}] {item['title']}")

        if exists(item["url"], item["title"]):
            print(" - Skip(ì¤‘ë³µ)")
            continue

        content = fetch_content(item["url"])

        if not is_it_related(item["title"], content):
            print(" - Skip(ë¹„IT)")
            continue

        ai = analyze_article(item["title"], content)
        save(item, ai, content)

        time.sleep(0.3)

