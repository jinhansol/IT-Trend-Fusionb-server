# backend/services/career_scraper.py

import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# ==========================================================
# ğŸ›¡ï¸ Anti-Detect Selenium Driver (ê³µí†µ ì‚¬ìš©)
# ==========================================================
def create_driver():
    options = Options()
    options.add_argument("--headless=new") # ìµœì‹  í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    
    # ë´‡ íƒì§€ ìš°íšŒ ì˜µì…˜ (ì¡ì½”ë¦¬ì•„/ì‚¬ëŒì¸ ê³µí†µ ì ìš©)
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )

    driver = webdriver.Chrome(options=options)

    # navigator.webdriver = undefinedë¡œ ì¡°ì‘ (ë§¤ìš° ì¤‘ìš”)
    driver.execute_cdp_cmd(
        "Page.addScriptToEvaluateOnNewDocument",
        {
            "source": """
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """
        },
    )
    return driver


# ==========================================================
# ğŸŸ¢ ì¡ì½”ë¦¬ì•„ (JobKorea)
# ==========================================================
def scrape_jobkorea(keyword, limit=20):
    print(f"ğŸš€ [JobKorea] Searching: {keyword}...")
    driver = create_driver()
    results = []
    
    try:
        url = f"https://www.jobkorea.co.kr/Search/?stext={keyword}&IsInLinkAction=False"
        driver.get(url)
        time.sleep(random.uniform(2, 3)) # ëœë¤ ëŒ€ê¸°

        # ë¬´í•œ ìŠ¤í¬ë¡¤ (ë°ì´í„° í™•ë³´)
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(3): # ë„ˆë¬´ ë§ì´í•˜ë©´ ëŠë ¤ì§€ë¯€ë¡œ 3~5íšŒ ì ë‹¹
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.5)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # BS4ë¡œ íŒŒì‹± (Seleniumë³´ë‹¤ ë¹ ë¥´ê³  ì•ˆì •ì )
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # ìµœì‹  DOM êµ¬ì¡° ë°˜ì˜
        cards = soup.select("div[data-sentry-component='CardJob']")
        
        for card in cards[:limit]:
            try:
                title_el = card.select_one("span[class*='Typography_variant_size18']")
                company_el = card.select_one("span[class*='Typography_variant_size16']")
                link_el = card.select_one("a[href*='/Recruit/']")
                
                if not title_el or not link_el:
                    continue

                title = title_el.get_text(strip=True)
                company = company_el.get_text(strip=True) if company_el else "Unknown"
                link = "https://www.jobkorea.co.kr" + link_el["href"] if link_el["href"].startswith("/") else link_el["href"]
                
                # ì§€ì—­ ì¶”ì¶œ
                location = "ì„œìš¸" # ê¸°ë³¸ê°’
                spans = card.select("span")
                for sp in spans:
                    text = sp.get_text(strip=True)
                    if any(x in text for x in ["ì„œìš¸", "ê²½ê¸°", "ì¸ì²œ", "êµ¬", "ì‹œ"]):
                        location = text
                        break

                results.append({
                    "source": "JobKorea",
                    "title": title,
                    "company": company,
                    "url": link,
                    "location": location,
                    "info": location  # í†µì¼ëœ í¬ë§·
                })
            except Exception:
                continue

    except Exception as e:
        print(f"âŒ [JobKorea] Error: {e}")
    finally:
        driver.quit()
    
    print(f"âœ… [JobKorea] Found {len(results)} jobs.")
    return results


# ==========================================================
# ğŸ”µ ì‚¬ëŒì¸ (Saramin)
# ==========================================================
def scrape_saramin(keyword, limit=20):
    print(f"ğŸš€ [Saramin] Searching: {keyword}...")
    driver = create_driver()
    results = []

    try:
        url = f"https://www.saramin.co.kr/zf_user/search?searchword={keyword}"
        driver.get(url)
        time.sleep(random.uniform(2, 3))

        # ìŠ¤í¬ë¡¤
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.5)

        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        job_cards = soup.select("div.item_recruit")
        
        for job in job_cards[:limit]:
            try:
                title_tag = job.select_one("h2.job_tit > a")
                company_tag = job.select_one("strong.corp_name > a")
                condition_tag = job.select_one("div.job_condition")

                if not title_tag:
                    continue

                title = title_tag.get_text(strip=True)
                company = company_tag.get_text(strip=True) if company_tag else "Unknown"
                link = "https://www.saramin.co.kr" + title_tag["href"]
                
                info_list = [span.get_text(strip=True) for span in condition_tag.select("span")] if condition_tag else []
                info_str = " Â· ".join(info_list)
                
                # ì§€ì—­ì€ info_listì˜ ì²« ë²ˆì§¸ ìš”ì†Œì¸ ê²½ìš°ê°€ ë§ìŒ
                location = info_list[0] if info_list else "ì§€ì—­ ì •ë³´ ì—†ìŒ"

                results.append({
                    "source": "Saramin",
                    "title": title,
                    "company": company,
                    "url": link,
                    "location": location,
                    "info": info_str
                })
            except Exception:
                continue

    except Exception as e:
        print(f"âŒ [Saramin] Error: {e}")
    finally:
        driver.quit()

    print(f"âœ… [Saramin] Found {len(results)} jobs.")
    return results


# ==========================================================
# âš¡ í†µí•© ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬)
# ==========================================================
def crawl_career_all(keyword="Python", limit_per_site=20):
    """
    JobKoreaì™€ Saraminì„ ë™ì‹œì— í¬ë¡¤ë§í•˜ì—¬ ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.
    """
    print("ğŸ”¥ [Career Scraper] Starting Parallel Crawling...")
    
    total_results = []
    
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ ë¸Œë¼ìš°ì €ë¥¼ ë™ì‹œì— ë„ì›€ (ì‹œê°„ ì ˆì•½)
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_jk = executor.submit(scrape_jobkorea, keyword, limit_per_site)
        future_sr = executor.submit(scrape_saramin, keyword, limit_per_site)
        
        for future in as_completed([future_jk, future_sr]):
            try:
                data = future.result()
                if data:
                    total_results.extend(data)
            except Exception as e:
                print(f"âš ï¸ Worker Error: {e}")

    # ê²°ê³¼ë¥¼ ëœë¤í•˜ê²Œ ì„ê±°ë‚˜, ìµœì‹ ìˆœ ì •ë ¬ ë“±ì„ í•  ìˆ˜ ìˆìŒ (ì—¬ê¸°ì„  ê·¸ëƒ¥ ë°˜í™˜)
    print(f"ğŸ‰ [Career Scraper] Total {len(total_results)} jobs collected.")
    return total_results