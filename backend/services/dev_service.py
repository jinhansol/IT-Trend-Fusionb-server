# backend/services/dev_service.py
# flake8: noqa

"""
ðŸ”¥ DevDashboard â€” GitHub Trending + README Raw ê¸°ë°˜ ì™„ì „ ì•ˆì • ë²„ì „
- GitHub API ì‚¬ìš© ì•ˆí•¨ â†’ 401 ì™„ì „ ì°¨ë‹¨
- READMEëŠ” raw.githubusercontent.comì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
- summary_kor: ì œëª© ê¸°ë°˜ ê°„ë‹¨ ìš”ì•½
- summary_detail: README ê¸°ë°˜ 3~5ì¤„ ìš”ì•½
"""

import os
import json
import time
import logging
import requests
import re
from bs4 import BeautifulSoup
import feedparser
from openai import OpenAI

logger = logging.getLogger("dev_service")
logger.setLevel(logging.INFO)

# ============================================================
# ðŸ”‘ ENV
# ============================================================
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_KEY)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Accept": "text/html,application/xhtml+xml",
}


# ============================================================
# ðŸ” Safe Request
# ============================================================
def safe_request(url, headers=None, timeout=10):
    try:
        res = requests.get(url, headers=headers or HEADERS, timeout=timeout)

        if res.status_code == 429:
            time.sleep(1.0)
            res = requests.get(url, headers=headers or HEADERS, timeout=timeout)

        if res.status_code != 200:
            logger.warning(f"[HTTP {res.status_code}] {url}")
            return None

        return res
    except Exception as e:
        logger.error(f"âŒ ìš”ì²­ ì‹¤íŒ¨ {url}: {e}")
        return None


# ============================================================
# JSON íŒŒì„œ
# ============================================================
def safe_json_parse(text):
    try:
        clean = re.sub(r"```json|```", "", text).strip()
        s = clean.find("[")
        e = clean.rfind("]") + 1
        clean = clean[s:e]
        return json.loads(clean)
    except:
        return []


# ============================================================
# ðŸ”¥ GitHub README Raw ê¸°ë°˜ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
# ============================================================
def fetch_repo_readme(full_repo_name):
    """
    ê°€ìž¥ ì•ˆì •ì ì¸ ë°©ë²•:
    https://raw.githubusercontent.com/{owner}/{repo}/{branch}/README.md
    APIë„ HTMLë„ ì•„ë‹˜ â†’ 401 ì—†ìŒ, Shadow DOM ì—†ìŒ.
    """
    if "/" not in full_repo_name:
        return ""

    owner, repo = full_repo_name.split("/")
    branches = ["main", "master"]

    for branch in branches:
        raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/README.md"
        res = safe_request(raw_url)

        if res:
            text = res.text.strip()
            if len(text) > 20:
                return text[:6000]  # 6KB ì œí•œ

    return ""


# ============================================================
# README ìš”ì•½
# ============================================================
def summarize_readme(full_name, readme):
    if not readme or len(readme) < 60:
        return ""

    prompt = f"""
ì•„ëž˜ëŠ” GitHub ì €ìž¥ì†Œ README ë‚´ìš©ìž…ë‹ˆë‹¤.
í•µì‹¬ ê¸°ëŠ¥Â·íŠ¹ì§•Â·ëª©ì ì„ í•œêµ­ì–´ 3~5ì¤„ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ì¶œë ¥(JSON):
{{
  "name": "{full_name}",
  "summary": "ìš”ì•½"
}}

README:
{readme}
"""

    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500,
            temperature=0.2,
        )

        raw = res.choices[0].message.content
        data = safe_json_parse(raw)

        if isinstance(data, dict):
            return data.get("summary", "")

        return ""
    except Exception as e:
        logger.error("âŒ README ìš”ì•½ ì‹¤íŒ¨:", e)
        return ""


# ============================================================
# GitHub Trending í¬ë¡¤ë§
# ============================================================
def fetch_github_trending(language: str = "", since: str = "daily"):
    base = "https://github.com/trending"
    url = f"{base}/{language}" if language else base
    url += f"?since={since}"

    res = safe_request(url)
    if not res:
        return []

    soup = BeautifulSoup(res.text, "html.parser")
    items = soup.select("article.Box-row")[:10]

    repos = []
    for it in items:
        link = it.select_one("h2 a")
        if not link:
            continue

        full_name = link.get("href", "").strip("/")

        stars_el = it.select_one("a.Link--muted[href*='stargazers']")
        stars = stars_el.text.strip() if stars_el else "0"

        repos.append({
            "full_name": full_name,
            "url": f"https://github.com/{full_name}",
            "stars": stars,
            "summary_kor": "",
            "summary_detail": "",
        })

    # ì œëª© ìš”ì•½
    repos = summarize_github_trending(repos)

    # README ìš”ì•½
    for r in repos:
        readme = fetch_repo_readme(r["full_name"])
        if readme:
            r["summary_detail"] = summarize_readme(r["full_name"], readme)

    return repos


# ============================================================
# ê°„ë‹¨ Trending ìš”ì•½ (ì œëª© ê¸°ë°˜)
# ============================================================
def summarize_github_trending(repos):
    if not repos:
        return repos

    names = [r["full_name"] for r in repos]
    joined = "\n".join([f"- {n}" for n in names])

    prompt = f"""
ì•„ëž˜ GitHub Trending ì €ìž¥ì†Œ ëª©ë¡ì˜ ëª©í‘œë¥¼ í•œêµ­ì–´ë¡œ 1~2ë¬¸ìž¥ì”© ìš”ì•½í•˜ì„¸ìš”.

ì¶œë ¥(JSON ë°°ì—´):
[
  {{"name": "owner/repo", "summary": "ìš”ì•½"}}
]

ëª©ë¡:
{joined}
"""

    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=600,
            temperature=0.2,
        )

        raw = res.choices[0].message.content
        data = safe_json_parse(raw)

        mapping = {d["name"].lower(): d["summary"] for d in data if "name" in d}

        for r in repos:
            r["summary_kor"] = mapping.get(r["full_name"].lower(), "")

        return repos

    except Exception:
        return repos


# ============================================================
# (ì„ íƒ) GitHub Repo ìƒì„¸ ì •ë³´ (API ì‚¬ìš©í•˜ì§€ë§Œ ë¡œê·¸ì¸ ëª¨ë“œì—ì„œë§Œ)
# ============================================================
def fetch_github_repo_updates(full_repo_name):
    """
    Personal ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©ë¨.
    APIëŠ” í† í° ìžˆì„ ë•Œë§Œ ë™ìž‘, ì—†ì–´ë„ ì•±ì´ ê¹¨ì§€ì§€ ì•Šê²Œ ë°©ì§€.
    """
    url = f"https://api.github.com/repos/{full_repo_name}"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/vnd.github.v3+json",
        "Authorization": f"token {os.getenv('GITHUB_TOKEN')}"
    }

    res = safe_request(url, headers=headers)
    if not res:
        return None

    d = res.json()
    return {
        "full_name": d.get("full_name"),
        "description": d.get("description"),
        "stars": d.get("stargazers_count"),
        "forks": d.get("forks_count"),
        "open_issues": d.get("open_issues_count"),
        "updated_at": d.get("updated_at"),
    }


# ============================================================
# Velog API
# ============================================================
def fetch_velog_popular_tags():
    url = "https://v2.velog.io/api/tags"
    res = safe_request(url)
    if not res:
        return []

    try:
        return [
            {"tag": t["name"], "count": t["count"]}
            for t in res.json()[:20]
        ]
    except:
        return []


def fetch_velog_posts_by_tag(tag):
    url = f"https://v2.velog.io/api/posts?tag={tag}"
    res = safe_request(url)
    if not res:
        return []

    try:
        arr = res.json()
    except:
        return []

    return [
        {
            "title": p["title"],
            "username": p["user"]["username"],
            "url": f"https://velog.io/@{p['user']['username']}/{p['url_slug']}",
            "likes": p["likes"],
            "thumbnail": p.get("thumbnail"),
            "summary": "",
        }
        for p in arr
    ]


def fetch_velog_trending_posts():
    url = "https://v2.velog.io/api/posts?sort=trending"
    res = safe_request(url)
    if not res:
        return []

    try:
        arr = res.json()
    except:
        return []

    return [
        {
            "title": p["title"],
            "username": p["user"]["username"],
            "url": f"https://velog.io/@{p['user']['username']}/{p['url_slug']}",
            "likes": p["likes"],
            "thumbnail": p.get("thumbnail"),
            "summary": p["title"],
        }
        for p in arr[:15]
    ]


def fetch_velog_rss(username):
    try:
        feed = feedparser.parse(f"https://v2.velog.io/rss/{username}")
        return [
            {
                "title": e.title,
                "url": e.link,
                "summary": e.summary,
                "published": e.published,
            }
            for e in feed.entries[:10]
        ]
    except:
        return []
