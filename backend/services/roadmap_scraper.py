# backend/services/roadmap_scraper.py

import requests
from bs4 import BeautifulSoup
import time

BASE_URL = "https://opentutorials.org"


# -----------------------------------------------------------
# ê¸°ë³¸ ìš”ì²­ í•¨ìˆ˜
# -----------------------------------------------------------
def _request(url):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
    }
    time.sleep(0.3)

    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()
    return resp.text


# -----------------------------------------------------------
# 1) ìƒí™œì½”ë”© í™ˆ(course/1)ì—ì„œ lesson URL ìˆ˜ì§‘
# -----------------------------------------------------------
def find_lessons_from_root():
    """ìƒí™œì½”ë”© ë©”ì¸(course/1) í˜ì´ì§€ì—ì„œ ëª¨ë“  ê°•ì˜ ë§í¬ íƒìƒ‰"""
    root_url = f"{BASE_URL}/course/1"
    print(f"   ğŸ“˜ Crawling ìƒí™œì½”ë”© ë©”ì¸: {root_url}")

    html = _request(root_url)
    soup = BeautifulSoup(html, "html.parser")

    lesson_links = []

    # ìƒí™œì½”ë”©ì˜ ëŒ€ë¶€ë¶„ ê°•ì˜ ë¦¬ìŠ¤íŠ¸ê°€ .lecture ì•ˆì— ìˆìŒ
    lecture_blocks = soup.select(".lecture a[href]")

    if lecture_blocks:
        print("   ğŸ” lecture ë¸”ë¡ì—ì„œ ê°•ì˜ ì¶”ì¶œ ì¤‘...")
        for a in lecture_blocks:
            href = a["href"]
            if "/course/" in href and href.count("/") >= 3:
                full_url = href if href.startswith("http") else BASE_URL + href
                if full_url not in lesson_links:
                    lesson_links.append(full_url)

    # lecture ë¸”ë¡ì„ ëª» ì°¾ìœ¼ë©´ ì „ì²´ ë§í¬ì—ì„œ fallback
    if not lesson_links:
        print("   âš ï¸ lecture ë¸”ë¡ ì—†ìŒ â†’ fallbackìœ¼ë¡œ ì „ì²´ ë§í¬ ê²€ìƒ‰")
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if "/course/" in href and href.count("/") >= 3:
                full_url = href if href.startswith("http") else BASE_URL + href
                if full_url not in lesson_links:
                    lesson_links.append(full_url)

    print(f"   â¤ Found {len(lesson_links)} lessons from root")
    return lesson_links


# -----------------------------------------------------------
# 2) ë‹¨ì¼ ê°•ì˜ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
# -----------------------------------------------------------
def scrape_opentutorials(url):
    try:
        print(f"      ğŸ•·ï¸ Scraping: {url}", end=" ")

        html = _request(url)
        soup = BeautifulSoup(html, "html.parser")

        title_tag = soup.find("meta", property="og:title")
        desc_tag = soup.find("meta", property="og:description")

        title = title_tag["content"] if title_tag else "ìƒí™œì½”ë”© ê°•ì˜"
        desc = desc_tag["content"] if desc_tag else ""

        print("OK")
        return {
            "title": title.strip(),
            "description": desc[:250].strip(),
            "resource_link": url,
            "thumbnail": None,
        }

    except Exception as e:
        print(f"FAILED ({e})")
        return {
            "title": "ìƒí™œì½”ë”© ê°•ì˜",
            "description": "",
            "resource_link": url,
            "thumbnail": None,
        }


# -----------------------------------------------------------
# 3) ìµœì¢… â†’ ìƒí™œì½”ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬ í¬ë¡¤ë§
# -----------------------------------------------------------
def crawl_life_coding_library():
    print("ğŸ”¥ ìƒí™œì½”ë”© ê°•ì˜ ìˆ˜ì§‘ ì‹œì‘...")

    lesson_urls = find_lessons_from_root()
    lessons = []

    # URL ë„ˆë¬´ ë§ì•„ì§€ë©´ 200ê°œ ì´í•˜ë¡œ ì˜ë¼ì„œ ì•ˆì •ì„± ìœ ì§€
    LESSON_LIMIT = 200
    lesson_urls = lesson_urls[:LESSON_LIMIT]

    for url in lesson_urls:
        lessons.append(scrape_opentutorials(url))

    print(f"\nğŸ“š Total lessons parsed: {len(lessons)}")
    return lessons
