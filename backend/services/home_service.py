# backend/services/home_service.py
# flake8: noqa
"""
ğŸ  í†µí•© Home Service
- ë‰´ìŠ¤ í¬ë¡¤ë§ (News Pipeline)
- í™ˆ í™”ë©´ ë°ì´í„° êµ¬ì„± (Charts, Serialization)
- íŠ¸ë Œë“œ ë¶„ì„ (Trend Summary, Recommendation)
- ê¸°ìˆ  ì‚¬ì „ (Dictionary) í¬í•¨
"""

import os
import json
import time
import re
import feedparser
import requests
from bs4 import BeautifulSoup
from collections import Counter, defaultdict
from datetime import datetime
from urllib.parse import urlparse, urljoin
from dotenv import load_dotenv
from openai import OpenAI
from sqlalchemy import or_

from database.mariadb import SessionLocal
from database.models import NewsFeed, UserProfile

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# ====================================================================
# ğŸ“š Tech Dictionary (ê¸°ìˆ  ë¶„ë¥˜ ì‚¬ì „ í†µí•©)
# ====================================================================
TECH_DICTIONARY = {
    "ai": [
        "ai", "machine learning", "deep learning", "ml", "llm", "large language model",
        "gpt", "chatgpt", "rag", "bert", "t5", "neural network", "computer vision",
        "generative ai", "íŒŒì¸íŠœë‹", "ë”¥ëŸ¬ë‹", "openai", "anthropic", "claude"
    ],
    "frontend": [
        "react", "next", "vue", "svelte", "angular", "tailwind", "javascript",
        "typescript", "html", "css", "ui"
    ],
    "backend": [
        "node", "express", "nestjs", "django", "flask", "fastapi", "spring", "java",
        "go", "golang", "rust", "serverless", "microservice"
    ],
    "mobile": [
        "ios", "swift", "android", "kotlin", "react native", "flutter"
    ],
    "data": [
        "big data", "spark", "hadoop", "flink", "etl", "elt", "ë¶„ì‚°ì²˜ë¦¬", "data pipeline"
    ],
    "cloud": [
        "aws", "azure", "gcp", "docker", "kubernetes", "k8s", "terraform",
        "github actions", "devops"
    ],
    "security": [
        "cybersecurity", "privacy", "encryption", "authentication", "authorization",
        "zero trust", "ê°œì¸ì •ë³´", "ë³´ì•ˆ"
    ],
}

STOPWORDS = [
    "ê¸°ìˆ ", "ì—…ê³„", "ê¸°ì—…", "ì„œë¹„ìŠ¤", "ì¶œì‹œ", "ë°œí‘œ", "ê°œë°œ", "ë„ì…",
    "ì—…ë°ì´íŠ¸", "ì‹œì¥", "ê´€ë ¨", "íš¨ê³¼", "ì—…ë¬´", "ì‚°ì—…", "ë¶„ì•¼"
]


# ====================================================================
# 1ï¸âƒ£ [News Pipeline] ë‰´ìŠ¤ í¬ë¡¤ë§ & ì €ì¥
# ====================================================================

# 20ê°œ IT RSS ë¦¬ìŠ¤íŠ¸
IT_FEEDS = [
    "https://www.zdnet.co.kr/Include/news.xml", "https://rss.etnews.com/Section903.xml",
    "https://www.itworld.co.kr/rss/all.xml", "https://www.ciokorea.com/rss/all.xml",
    "https://koreaittimes.com/rss/allArticle.xml", "https://www.ddaily.co.kr/news/rss/allArticle.xml",
    "https://www.bloter.net/rss", "https://www.boannews.com/media/rss.xml",
    "https://techcrunch.com/feed/", "https://www.wired.com/feed/category/business/latest/rss",
    "https://www.theverge.com/rss/index.xml", "http://feeds.arstechnica.com/arstechnica/index",
    "https://venturebeat.com/feed/", "https://feeds.infoq.com/",
    "http://rss.slashdot.org/Slashdot/slashdotMain",
]

# HTML Fallback ë„ë©”ì¸ ë§¤í•‘
FALLBACK_MAP = {
    "zdnet.co.kr": "https://www.zdnet.co.kr/news/",
    "etnews.com": "https://www.etnews.com/news/",
    "itworld.co.kr": "https://www.itworld.co.kr/",
    "techcrunch.com": "https://techcrunch.com/",
    "theverge.com": "https://www.theverge.com/tech",
}

def fetch_rss(feed_url):
    parsed = feedparser.parse(feed_url)
    items = []
    for e in parsed.entries:
        title = e.get("title", "").strip()
        link = e.get("link", "").strip()
        if title and link:
            items.append({"title": title, "url": link})
    return items

def fetch_html_items(feed_url):
    domain = urlparse(feed_url).netloc.replace("www.", "")
    base_url = next((v for k, v in FALLBACK_MAP.items() if k in domain), None)
    if not base_url:
        return []

    try:
        res = requests.get(base_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        items = []
        for a in soup.find_all("a", href=True)[:20]:
            title = a.get_text(strip=True)
            link = a["href"]
            if len(title) > 10:
                full = link if link.startswith("http") else urljoin(base_url, link)
                items.append({"title": title, "url": full})
        return items
    except:
        return []

def fetch_content(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        for sel in ["article", "main", "#articleBody", ".article_body", ".post-content"]:
            t = soup.select_one(sel)
            if t: return t.get_text(separator="\n").strip()
        return soup.get_text(separator="\n").strip()[:3000]
    except:
        return ""

def analyze_article(title, content):
    """GPT-4o-minië¥¼ ì´ìš©í•œ ë‰´ìŠ¤ ìš”ì•½ ë° ë¶„ë¥˜"""
    if len(content) < 50:
        content = f"ì œëª© ê¸°ë°˜ ìš”ì•½: {title}"

    prompt = f"""
    [ì œëª©] {title}
    [ë³¸ë¬¸] {content[:2000]}
    
    ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ JSONì„ ìƒì„±í•˜ì„¸ìš”:
    {{
      "summary": "í•œêµ­ì–´ 3ë¬¸ì¥ ìš”ì•½",
      "category": "ai|cloud|security|backend|frontend|data|etc",
      "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
    }}
    """
    try:
        res = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        return json.loads(res.choices[0].message.content)
    except:
        return {"summary": title, "category": "etc", "keywords": ["IT"]}

def run_news_pipeline():
    print("ğŸ”¥ [Home] News Pipeline Started...")
    all_items = []
    
    # 1. ìˆ˜ì§‘
    for feed in IT_FEEDS:
        items = fetch_rss(feed)
        if len(items) < 3:
            items.extend(fetch_html_items(feed))
        all_items.extend(items[:3])

    # 2. ì €ì¥
    db = SessionLocal()
    count = 0
    for item in all_items:
        if db.query(NewsFeed).filter(NewsFeed.url == item["url"]).first():
            continue
            
        content = fetch_content(item["url"])
        ai_data = analyze_article(item["title"], content)
        
        news = NewsFeed(
            title=item["title"],
            summary=ai_data["summary"],
            content=content,
            category=ai_data["category"],
            keywords=json.dumps(ai_data["keywords"], ensure_ascii=False),
            url=item["url"],
            source=urlparse(item["url"]).netloc,
            published_at=datetime.utcnow(),
            created_at=datetime.utcnow(),
        )
        db.add(news)
        count += 1
        
    try:
        db.commit()
        print(f"âœ… [Home] {count} new articles saved.")
    except Exception as e:
        print(f"âŒ [Home] Save Error: {e}")
        db.rollback()
    finally:
        db.close()


# ====================================================================
# 2ï¸âƒ£ [Chart & Data] í™ˆ í™”ë©´ ë°ì´í„° êµ¬ì„±
# ====================================================================

def get_date_key(date):
    return date.strftime("%Y-%m-%d")

def detect_category(text: str):
    t = text.lower()
    scores = defaultdict(int)
    for cat, kws in TECH_DICTIONARY.items():
        for kw in kws:
            if kw in t: scores[cat] += 1
    return max(scores, key=scores.get) if scores else "Other"

def extract_keywords(text: str):
    t = re.sub(r"[^a-zA-Z0-9ê°€-í£\s]", " ", text.lower())
    words = [w for w in t.split() if len(w) > 1 and w not in STOPWORDS]
    return words

def build_charts(news_items):
    """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œìš© ì°¨íŠ¸ ë°ì´í„° ìƒì„±"""
    if not news_items:
        return {"category_ratio": [], "keyword_ranking": [], "weekly_trend": []}

    cat_counter = Counter()
    kw_counter = Counter()
    daily_trend = defaultdict(lambda: defaultdict(int))

    for n in news_items:
        text = f"{n.title} {n.summary}"
        cat = detect_category(text)
        cat_counter[cat] += 1
        
        try:
            kws = json.loads(n.keywords)
            kw_counter.update(kws)
        except:
            kw_counter.update(extract_keywords(text))

        if n.published_at:
            date_key = get_date_key(n.published_at)
            daily_trend[date_key][cat] += 1

    weekly_trend = []
    for d in sorted(daily_trend.keys()):
        data = {"date": d}
        data.update(daily_trend[d])
        weekly_trend.append(data)

    return {
        "category_ratio": [{"category": k, "count": v} for k, v in cat_counter.items()],
        "keyword_ranking": [{"keyword": k, "count": v} for k, v in kw_counter.most_common(20)],
        "weekly_trend": weekly_trend,
    }

def serialize_news(item: NewsFeed):
    return {
        "id": item.id,
        "title": item.title,
        "summary": item.summary,
        "source": item.source,
        "url": item.url,
        "published_at": item.published_at,
        "category": item.category,
    }


# ====================================================================
# 3ï¸âƒ£ [Trend Logic] íŠ¸ë Œë“œ ì¶”ì²œ ë° ìš”ì•½
# ====================================================================

async def get_trend_recommendations(user_id: int):
    """ì‚¬ìš©ì ê´€ì‹¬ì‚¬ ê¸°ë°˜ íŠ¸ë Œë“œ ì¶”ì²œ"""
    db = SessionLocal()
    user = db.query(UserProfile).filter(UserProfile.id == user_id).first()
    
    if not user or not user.interest_topics:
        db.close()
        return {"message": "ê´€ì‹¬ì‚¬ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."}

    results = []
    for keyword in user.interest_topics:
        news_items = (
            db.query(NewsFeed)
            .filter(NewsFeed.title.ilike(f"%{keyword}%"))
            .order_by(NewsFeed.published_at.desc())
            .limit(3)
            .all()
        )
        
        if not news_items:
            continue
            
        titles = [n.title for n in news_items]
        prompt = f"í‚¤ì›Œë“œ [{keyword}] ê´€ë ¨ ë‰´ìŠ¤ ì œëª©ë“¤ì…ë‹ˆë‹¤:\n" + "\n".join(titles) + "\ní•µì‹¬ íŠ¸ë Œë“œë¥¼ 2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜."
        
        try:
            res = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}]
            )
            summary = res.choices[0].message.content.strip()
            results.append({"keyword": keyword, "summary": summary})
        except:
            pass

    db.close()
    return {"recommendations": results}